%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,leqno]{article}
\usepackage[latin9]{inputenc}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}
\usepackage{breakurl}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% HW2 high dimensional data

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{rotating}
\usepackage{rotating}
%\usepackage{babel}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\macheps}{\epsilon_{\mbox{\scriptsize mach}}}
\let\oldhat\hat
\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\hat}[1]{\oldhat{{#1}}}
\def\rp{\ensuremath \mathbb{R}^p}
\def\rpp{\ensuremath \mathbb{R}^{p \times p}}
\def\s{\ensuremath\Sigma}
\def\om{\ensuremath\Omega}
\def\pd{\ensuremath\mathbb{P}^+}
\def\pg{\ensuremath\mathbb{P}_{{G}}}
\def\E{\ensuremath\mathbb{E}}
\def\normdist[#1]#2{\ensuremath \sim \mathcal{N} (#1,#2) }
\def\ndist1{\ensuremath \sim \mathcal{N}  (\mu, \sigma)}
\def\ndistvec{\ensuremath \sim \mathcal{N}_p ( {\mu},  {\Sigma})}
\def\lra{\ensuremath\Leftrightarrow}
\def\stackrel#1#2{\mathrel{\mathop{#2}\limits^{#1}}}
\newcommand{\ind}{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newtheorem{thm}{}[]
\newtheorem{lemma}{}[]
\newtheorem{defn}[thm]{}\newcommand{\sign}{\mathrm{sign}}
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\dist}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}

\makeatother

\begin{document}
\pagestyle{fancy} \lhead{TA,RB,SR,AS} \rhead{STA7934}

\begin{center}
\textbf{\large{}Homework 3 - Analysis of High Dimensional Data} \\
 \textit{{Tavis Abrahamsen, Ray Bai, Syed Rahman and Andrey Skripnikov}}
\\
 
\par\end{center}


\paragraph{}

Consider the Gaussian graphical model as presented in class. In a
seminal paper Meinshausen and Buhlmann (Annals of Statistics, 2006)
showed that one can estimate the model by using node-wise penalized
(lasso) regression, followed by post-processing to obtain a symmetric
and positive definite estimate of $\Omega$. 
\begin{enumerate}
\item Using your code from HW 2, estimate a Gaussian graphical model using
the lasso node-wise regression method. 
\item Apply your algorithms to a chain network, a nearest neighbor network,
and a scale free network of size p = 25 with n = 150 observations.
Set the density level of your edge set at 10 (see Section 4 of Guo
et al. (Biometrika, 2011)). 
\item Replicate the setting in (2) 10 times and provide an estimate of the
false positive and false negative rates, as well as the Frobenius
norm of the difference between your estimate and the true $\Omega$. 
\end{enumerate}

\paragraph{}

The Meinshausen-Buhlmann method to estimate the partial correlations
consists of running the following $lasso$ regressions for $i=1,...,p$:
\begin{align}
\min_{\tilde{\beta}_{i}}\frac{1}{2}\norm{X_{i}-\sum_{j\not=i}\beta_{ji}X_{j}}_{2}^{2}+\lambda\norm{\tilde{\beta}_{i}}_{1}\label{eq:mb}
\end{align}
where $\tilde{\beta}_{i}=(\beta_{j})_{j\not=i,1\leq j\leq p}$. Our
goal in this assignment is to estimate the sparsity patterns and the
partial correlation matrices corresponding to chain graphs, nearest
neighbor networks and Barabasi graphs with density equal to approximately
$10\%$. Without some post-processing this method doesn't usually
produce positive definite estimates and hence is mainly used as a
method of estimating the sparsity pattern as opposed to the concentration
matrix itself.

To generate $\om$ corresponding to a chain graph we first construct
a $25\times25$ covariance matrix, $\Sigma$ where the $(i,j)$th
element of $\Sigma$, $\sigma_{i,j}$, is defined as 
\begin{align*}
\sigma_{i,j} & =e^{-\abs{s_{i}-s_{j}}/2}\quad\text{ for }s_{1}<s_{2}<...<s_{p}\\
\text{ and }s_{j}-s_{j-1} & \sim U(0.5,1)\quad\text{ for }j=2,...,p.
\end{align*}
Then we set $\Omega=\Sigma^{-1}$. Our precision matrix thus constructed
is a tridiagonal matrix. In the case of the nearest neighbor network,
we generate 25 points randomly on the unit square $(0,1)\times(0,1)$.
We calculate all $p(p-1)/2$ pairwise distances and find $m$ nearest
neighbors of each point in terms of this distance. The nearest neighbor
network is obtained by linking any two points that are $m$-nearest
neighbors of each other. The integer $m=2$ is chosen for our study.

To find the solutions to the $lasso$ regressions in \ref{eq:mb}
we use a proximal gradient algorithm using a constant step size, denoted
by $t$, of 0.0001. The algorithm is as follows: 
\begin{align*}
\beta^{k+1} & =\beta^{k}-t\nabla\frac{1}{2}\norm{X_{i}-\sum_{j\not=i}\beta_{ji}^{k}X_{j}}_{2}^{2}\\
\beta^{k+1} & =S_{t\lambda}(\beta^{k+1})
\end{align*}
where $S_{\lambda}(.)$ denotes the soft-thresholding operator. The
next step was to estimate the optimal $\lambda$ for each graph type.
Let 
\begin{align*}
F(\hat{\om}) & =\frac{\norm{\om-\hat{\om}}_{F}^{2}}{\norm{\om}_{F}^{2}}\\
FP(\hat{\om}) & =\frac{\sum_{i,j}I(\omega_{ij}=0,\hat{\omega}_{ij}\not=0)}{\sum_{i,j}I(\omega_{ij}=0)}\\
FN(\hat{\om}) & =\frac{\sum_{i,j}I(\omega_{ij}\not=0,\hat{\omega}_{ij}=0)}{\sum_{i,j}I(\omega_{ij}\not=0)}\\
\end{align*}
We did this by comparing $F,FP$ and $FN$ at various $\lambda$ between
0 and 114. In general, $F$ and $FP$ decreased as $\lambda$ increased
while the $FN$ increased with $\lambda$ and therefore there usually
wasn't any ideal $\lambda$ all these measures pointed to. In the
case of the chain graph $\lambda=55$ worked well because both the
$FN$ and $FP$ were equal to 0 as Figure \ref{fig:chain} illustrates.
Also, at this value of $\lambda$, we were able to recover the correct
sparsity pattern as shown in Figure \ref{fig:chaingraphs}. For the
other two graphs we chose $\lambda$ to minimize $FN$ given that
the $FP$ was below $0.1$. For the nearest neighbor network, this
happens at around $\lambda=25$.

Finally let 
\begin{align*}
F_{K}(\hat{\om}) & =\frac{1}{K}\sum_{k=1}^{K}\frac{\norm{\om^{k}-\hat{\om^{k}}}_{F}^{2}}{\norm{\om^{k}}_{F}^{2}}\\
FP_{K}(\hat{\om}) & =\frac{1}{K}\sum_{k=1}^{K}\frac{\sum_{i,j}I(\omega_{ij}^{k}=0,\hat{\omega}_{ij}^{k}\not=0)}{\sum_{i,j}I(\omega_{ij}^{k}=0)}\\
FN_{K}(\hat{\om}) & =\frac{1}{K}\sum_{k=1}^{K}\frac{\sum_{i,j}I(\omega_{ij}^{k}\not=0,\hat{\omega}_{ij}^{k}=0)}{\sum_{i,j}I(\omega_{ij}^{k}\not=0)}\\
\end{align*}
We set $K=10$ and replicate the numerical simulations tens times
and calculate $F_{K},FP_{K}$ and $FN_{K}$ for our estimate. A disadvantage
of this method is that all the $lasso$ regressions are independent
with a consequence being the loss of positive definiteness of $\hat{\om}$.
To calculate a positive definite estimate for $\om$, denoted by $\hat{\om}^{pd}$,
we can follow steps 1 and 2 as outlined below. 
\begin{enumerate}
\item If $i\not=j$, set $\om_{ij}=\max\{\om_{ij},\om_{ji}\}$. 
\item While $\Lambda_{min}(\om)<0$, then $\om=\om-\Lambda_{min}(\om)*I$ 
\end{enumerate}
Let $F_{K}^{pd}=F_{K}(\hat{\om}^{pd})$ where $\hat{\om}^{pd}$ is
positive definite. As Table \ref{tab:compare} shows most of the results
were consistent with our initial calculations. For the chain graph,
the prediction of the sparsity pattern tends to be very accurate,
but the Frobenius Norm Loss can be higher than for other graph types.
In the case of the Nearest Neighbor Network, the Frobenius Norm loss
is smaller, but the False Negative rate is much higher indicating
that this method doesn't work as well in predicting the sparsity pattern
for these type of graphs. 
\begin{table}
\centering{}%
\begin{tabular}{rccc}
\toprule 
 & Chain & Nearest Neighbor & Barabasi\tabularnewline
\midrule 
$F_{10}^{pd}$ & 0.7137008 & 0.3130917 & xxx\tabularnewline
$F_{10}$ & 0.6640735 & 0.3137885 & xxx\tabularnewline
$FP_{10}$ & 0.0001897533 & 0.07741935 & xxx\tabularnewline
$FN_{10}$ & 0 & 0.6784593 & xxx\tabularnewline
\bottomrule
\end{tabular}\protect\caption[]{A comparison of the Meinshausen-Buhlmann method for Chain, Nearest
Neighbor and Barabasi graphs}
\label{tab:compare} 
\end{table}



\end{document}
